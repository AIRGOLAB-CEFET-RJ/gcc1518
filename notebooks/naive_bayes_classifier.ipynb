{"cells":[{"cell_type":"markdown","metadata":{"id":"GBmQS9s945nl"},"source":["# Naive Bayes Classifier\n","\n","Naive Bayes Classifier is a probabilistic classification model. The model generated by the Naive Bayes algorithm is a set of *probability values* that are estimated from a training dataset."]},{"cell_type":"markdown","metadata":{"id":"UU0R25fD2rFT"},"source":["## Naive Bayes Classifier - relation to Bayes Theorem\n","\n","Naive Bayes Classifier consists of two steps, which are described below. Formally, let $X$ be the trainin dataset. Also consider that $c_1, c_2, \\ldots, c_k$ are the classes of the problem (i.e., the possible values ​​of the target) and that $\\mathbf{x} = [x_1, x_2, ..., x_n]$ is a new example that should be classified. Let $a_1, a_2, ..., a_n$ be the values for the predictive features $x_1, x_2, ..., x_n$, respectively. \n","\n","The term \"bayesian\" comes from [Thomas Bayes](https://en.wikipedia.org/wiki/Thomas_Bayes), a British Presbyterian minister who lived in the 18th century and who formulated the famous [Bayes' theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem).\n","\n","$$\n","\\Pr(c_j \\mid x_1, x_2, \\dots, x_n) = \\frac{\\Pr(c_j) \\Pr (x_1, x_2, \\dots x_n \\mid c_j)} {\\Pr(x_1, x_2, \\dots, x_n)}\n","$$\n","\n","The following is a description of each term in the above expression, in the context of a [statistical classification](https://en.wikipedia.org/wiki/Statistical_classification) task:\n","\n","* $\\Pr(c_j \\mid x_1, \\dots, x_n)$ represents the probability of the class $c$, given the values ​​of the attributes of the example $\\mathbf{x}$. These terms, called **posterior probabilities**, are what must be determined (learned) by the algorithm. There is one probability value for each value $c_j$ the target can assume ($c_j \\in \\mathcal{C}$). \n","\n","* $\\Pr(x_1, \\dots x_n \\mid c_j)$ represents the probability that a specific combination of values ​​$x_1, \\dots, x_n$ will occur in examples associated with value $c_j$ of the target attribute. Thesse terms are called **likelihoods**.\n","\n","* $\\Pr(c_j)$ represents the probability that an example selected at random belongs to a given class (i.e., belongs to a given value of the target attribute $y$). This term is called **prior probability**.\n","\n","* $\\Pr(x_1, x_2, \\dots, x_n) $ represents the probability that a given combination of values ​​$x_1, x_2, \\dots, x_n$ will occur in an example selected at random.\n","\n","Notice that $\\Pr(c_j \\mid x_1, \\dots, x_n)$ are the probability values that we want to determine. If we know these probability values, we can predict the class of the example $[x_1, \\dots, x_n]$ by using the following expression:\n","\n","$$\n","c_{map} = \\underset{c_{j} \\in \\mathcal{C}}{\\operatorname{argmax}} \\left[\\Pr(c_j \\mid x_1, \\dots, x_n) \\right] = \\underset{c_{j} \\in \\mathcal{C}}{\\operatorname{argmax}} \\left[\\frac{\\Pr(c_j) \\Pr (x_1, x_2, \\dots x_n \\mid c_j)} {\\Pr(x_1, x_2, \\dots, x_n)} \\right]\n","$$\n","\n","The above expression uses the $\\operatorname{argmax}$ operator. The interpretation is simple: we are going to predict the class $c_{map}$ that is associated to the maximum value resulting from computing the expression in brackets.\n","\n","In the above expression, notice that $\\Pr(x_1, x_2, \\dots, x_n)$ does not depende on $c_j$. Hence, we can simplify the computation of the $\\operatorname{argmax}$ operator:\n","\n","$$\n","c_{map} = \\underset{c_{j} \\in \\mathcal{C}}{\\operatorname{argmax}} \\left[\\frac{\\Pr(c_j) \\Pr (x_1, x_2, \\dots x_n \\mid c_j)} {\\Pr(x_1, x_2, \\dots, x_n)} \\right] = \\underset{c_{j} \\in \\mathcal{C}}{\\operatorname{argmax}} \\left[\\Pr(c_j) \\Pr(x_1, x_2, \\dots x_n \\mid c_j) \\right]\n","$$"]},{"cell_type":"markdown","metadata":{},"source":["## Naive Bayes Classifier - steps\n","\n","From the discussion provided above, we can summarize the NBC algorithm in two steps, which are described below. \n","\n","1. Calculate the posterior probabilities $\\Pr(c_j \\mid \\mathbf{x})$, $j = 1,2, \\ldots, k $\n","2. Classify $\\mathbf{x}$ as being of class $c_{map}$ such that $\\Pr(c_{max} \\mid \\mathbf{x})$ is maximum.\n","\n","\n","By inspecting the expression for $c_{map}$, we can conclude that we need to compute two sets of probability values:\n","\n","- $\\Pr(c_j)$, $c_j \\in \\mathcal{C}$\n","- $\\Pr(x_1, x_2, \\dots x_n \\mid c_j)$, $c_i \\in \\mathcal{C}$"]},{"cell_type":"markdown","metadata":{"id":"ssZtdQAdM119"},"source":["## Naive Bayes Classifier - computing priors and likelihoods\n","\n","Consider that $c_1, c_2, \\ldots, c_k$ are the classes of the problem (i.e., the possible values ​​of the target) and that $\\mathbf{x} = [x_1, x_2, ..., x_n]$ is a new example that should be classified. Let $a_1, a_2, ..., a_n$ be the values for the predictive features $\\mathbf{x} = [x_1, x_2, ..., x_n]$, respectively. \n","\n","More concretely, consider that $\\mathbf{x}$ is the following:\n","\n","$$\n","\\mathbf{x} = [\\operatorname{outlook} = \\text{Sunny}, \\operatorname{temp} = \\text{Hot}, \\operatorname{humidity} = \\text{High}, \\operatorname{wind} = \\text{Weak}]\n","$$\n","\n","Hence, we are provided information about the weather conditions of a given day (represented by $\\mathbf{x}$), and we want to answer (*predict*) whether or not this is a good day to play tennis. \n","\n","There are only two possibilities, Yes (play=Yes) or No (play=No). Therefore, let us define two probability values:\n","\n","$$\n","\\begin{align*}\n","\\Pr(\\text{play = Yes} \\mid \\mathbf{x}) & \\text{: probability that } \\mathbf{x} \\text{ is a good day to play.}\\\\\n","\\Pr(\\text{play = No} \\mid \\mathbf{x}) & \\text{: probability that } \\mathbf{x} \\text{ is NOT a good day to play.}\n","\\end{align*}\n","$$\n","\n","Notice that, if we know the probabilites values above, the problem is solved. That is because we can use these values to make our decision: if $\\Pr(\\text{play = Yes} > \\Pr(\\text{play = No})$, then we predict that $\\mathbf{x}$ is a good day to play tennis, that is, we predict play = Yes. Otherwise, we predict play = No.\n","\n","But, how can we compute those probability values? The answer lies in the Bayes Rule. To compute $\\Pr(\\text{play = Yes} \\mid \\mathbf{x})$, we use Bayes Rules and write:\n","\n","$$\n","\\begin{align*}\n","\\Pr(\\text{play = Yes} \\mid \\mathbf{x}) & = \\frac{\\Pr(\\mathbf{x} \\mid \\text{play = Yes}) \\times \\Pr(\\text{play} = \\text{Yes})}{\\Pr(\\mathbf{x})} = \\\\\n","&=  \\frac{\\Pr(\\text{outlook} = \\text{Sunny}, \\operatorname{temp} = \\text{Hot}, \\operatorname{humidity} = \\text{High}, \\operatorname{wind} = \\text{Weak} \\mid \\text{play = Yes}) \\times \\Pr(\\text{play = Yes})}{\\Pr(\\text{outlook} = \\text{Sunny}, \\operatorname{temp} = \\text{Hot}, \\operatorname{humidity} = \\text{High}, \\operatorname{wind} = \\text{Weak})}\n","\\end{align*}\n","$$\n","\n","To compute $\\Pr(\\text{play = No} \\mid \\mathbf{x})$, we write a similar expression:\n","\n","$$\n","\\begin{align*}\n","\\Pr(\\text{play = No} \\mid \\mathbf{x}) & = \\frac{\\Pr(\\mathbf{x} \\mid \\text{play = No}) \\times \\Pr(\\text{play} = \\text{No})}{\\Pr(\\mathbf{x})} = \\\\\n","&=  \\frac{\\Pr(\\text{outlook} = \\text{Sunny}, \\operatorname{temp} = \\text{Hot}, \\operatorname{humidity} = \\text{High}, \\operatorname{wind} = \\text{Weak} \\mid \\text{play = No}) \\times \\Pr(\\text{play = No})}{\\Pr(\\text{outlook} = \\text{Sunny}, \\operatorname{temp} = \\text{Hot}, \\operatorname{humidity} = \\text{High}, \\operatorname{wind} = \\text{Weak})}\n","\\end{align*}\n","$$\n","\n","By looking at the two expressions above, it seems there are several probability values we need to compute using the provided dataset. Let us list each one of them:\n","\n","1. $\\Pr(\\text{play = Yes})$\n","2. $\\Pr(\\text{play = No})$\n","3. $\\Pr(\\text{outlook} = \\text{Sunny}, \\operatorname{temp} = \\text{Hot}, \\operatorname{humidity} = \\text{High}, \\operatorname{wind} = \\text{Weak} \\mid \\text{play = No})$\n","4. $\\Pr(\\text{outlook} = \\text{Sunny}, \\operatorname{temp} = \\text{Hot}, \\operatorname{humidity} = \\text{High}, \\operatorname{wind} = \\text{Weak} \\mid \\text{play = Yes})$\n","5. $\\Pr(\\text{outlook} = \\text{Sunny}, \\operatorname{temp} = \\text{Hot}, \\operatorname{humidity} = \\text{High}, \\operatorname{wind} = \\text{Weak})$\n","\n","Items 1 and 2 are easy to compute, and we arlready know how to estimate them. Another good news is that we don't actually need to compute item 5, since this expression appears as denominator of both $\\Pr(\\text{play = yes} \\mid \\mathbf{x})$ and $\\Pr(\\text{play = No} \\mid \\mathbf{x})$. We are left with items 3 and 4. Lets us apply the definition of [conditional probability](https://en.wikipedia.org/wiki/Conditional_probability) to one of these expressions (item 4):\n","\n","$$\n","\\begin{align*}\n","\\Pr(\\text{outlook} = \\text{Sunny}, \\operatorname{temp} = \\text{Hot}, \\operatorname{humidity} = \\text{High}, \\operatorname{wind} = \\text{Weak} \\mid \\text{play = Yes}) = \\\\\n","\\Pr(\\text{outlook = Sunny} \\mid \\text{play = Yes}) \\times \\\\ \n","\\times\\Pr(\\operatorname{temp} = \\text{Hot} \\mid \\text{outlook} = \\text{Sunny}, \\text{play = Yes}) \\times \\\\\n","\\times\\Pr(\\operatorname{humidity} = \\text{High} \\mid \\text{outlook} = \\text{Sunny}, \\operatorname{temp} = \\text{Hot}, \\text{play = Yes}) \\times\\\\\n","\\times\\Pr(\\operatorname{wind} = \\text{Weak} \\mid \\text{outlook} = \\text{Sunny}, \\operatorname{temp} = \\text{Hot}, \\operatorname{humidity} = \\text{High}, \\text{play = Yes})\n","\\end{align*}\n","$$\n","\n","By looking at the above expression, it seems that we have to compute a lot of probability estimates from the available data. That is when the naive assumption of Naive Bayes Classifier comes in handy. This algorithm assumes that one attribute is conditionally independent from each other, once we know the value of the class. \n","\n","> *Conditional independence*. The term [conditional independence](https://www.probabilitycourse.com/chapter1/1_4_4_conditional_independence.php) corresponds to a somewhat advanced concept in Probability Theory. Given three variables A, B, and C. We say that variables A and B are conditionally independet given the variable C if and only if knowing the value of C makes A and B independent of each other.\n","$$\n","\\Pr(A \\mid B, C) = \\Pr(A \\mid C)\n","$$\n","\n","The term *naive* stems from the fact that Naive Bayes considers that the attributes are conditionally independent given the class. When considering this hypothesis, the computation of the conditional probabilities can be simplified. Mathematically, we have:\n","$$\n","\\Pr(x_1, x_2, \\dots x_n \\mid y) = \\Pr(x_1 \\mid y) \\times \\Pr(x_2 \\mid y) \\times \\ldots \\times \\Pr(x_n \\mid y)\n","$$\n","\n","In many practical cases, this statistical independence between predictors does not exist. For example, consider a dataset with information about customers of a company. Also consider that each customer is represented by the following features: *weight*, *education*, *salary*, *age*, etc. In this dataset, the values ​​of the first three feature are correlated with values ​​of the age. In this case, at least in theory, the use of Naive Bayes would overestimate the effect of the age feature. However, practice shows that Naive Bayes is quite effective even in cases where the predictive features are not statistically independent.\n","\n","Anyway, assuming the naive hypothesis is true, we can simplify the Bayes formula:\n","\n","$$\n","\\Pr(y \\mid x_1, x_2, \\dots, x_n) \\propto \\Pr(y) \\times \\Pr(x_1 \\mid y) \\times \\Pr(x_2 \\mid y) \\times \\ldots \\Pr(x_n \\mid y)\n","$$\n","\n","\n","Naive Bayes Classifier uses this assumption of conditional independence to simplify the computation of the probability estimates tha should be produced. By applying this assumption to the estimates above, we end up with the following:\n","\n","$$\n","\\begin{align*}\n","\\Pr(\\text{outlook} = \\text{Sunny}, \\operatorname{temp} = \\text{Hot}, \\operatorname{humidity} = \\text{High}, \\operatorname{wind} = \\text{Weak} \\mid \\text{play = Yes}) = \\\\\n","\\Pr(\\text{outlook = Sunny} \\mid \\text{play = Yes}) \\times \\\\ \n","\\times\\Pr(\\operatorname{temp} = \\text{Hot} \\mid \\text{play = Yes}) \\times \\\\\n","\\times\\Pr(\\operatorname{humidity} = \\text{High} \\mid \\text{play = Yes}) \\times\\\\\n","\\times\\Pr(\\operatorname{wind} = \\text{Weak} \\mid \\text{play = Yes})\n","\\end{align*}\n","$$\n","\n","We can write a similar expression for $\\text{play = No}$:\n","\n","$$\n","\\begin{align*}\n","\\Pr(\\text{outlook} = \\text{Sunny}, \\operatorname{temp} = \\text{Hot}, \\operatorname{humidity} = \\text{High}, \\operatorname{wind} = \\text{Weak} \\mid \\text{play = No}) = \\\\\n","\\Pr(\\text{outlook = Sunny} \\mid \\text{play = No}) \\times \\\\ \n","\\times\\Pr(\\operatorname{temp} = \\text{Hot} \\mid \\text{play = No}) \\times \\\\\n","\\times\\Pr(\\operatorname{humidity} = \\text{High} \\mid \\text{play = No}) \\times\\\\\n","\\times\\Pr(\\operatorname{wind} = \\text{Weak} \\mid \\text{play = No})\n","\\end{align*}\n","$$\n","\n","$$\n","\\Pr(c_i \\mid \\mathbf{x}), \\, 1 \\leq i \\leq n\n","$$"]},{"cell_type":"markdown","metadata":{},"source":["## Estimating priors and likelihoods from data\n","\n","The probabilities are actually estimated from a training dataset by the Naive Bayes algorithm. These estimates are computed by counting the occurrences of values in a given feature, either separately or in conjunction with values of the target.\n","\n","Let us see an example of how probability estimates can be computed from data provided as a dataset. For this, consider the [Play Tennis dataset](https://www.kaggle.com/fredericobreno/play-tennis), which is another toy dataset with four predictors (`outlook`, `temp`, `humidity`, and `wind`) and fourteen examples. The target (`play`) is binary. Each example provides data about the weather condition in a particular day. Therefore, the classification task is to predict whether a given day is appropriate to play tennis or not."]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>day</th>\n","      <th>outlook</th>\n","      <th>temp</th>\n","      <th>humidity</th>\n","      <th>wind</th>\n","      <th>play</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>D1</td>\n","      <td>Sunny</td>\n","      <td>Hot</td>\n","      <td>High</td>\n","      <td>Weak</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>D2</td>\n","      <td>Sunny</td>\n","      <td>Hot</td>\n","      <td>High</td>\n","      <td>Strong</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>D3</td>\n","      <td>Overcast</td>\n","      <td>Hot</td>\n","      <td>High</td>\n","      <td>Weak</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>D4</td>\n","      <td>Rain</td>\n","      <td>Mild</td>\n","      <td>High</td>\n","      <td>Weak</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>D5</td>\n","      <td>Rain</td>\n","      <td>Cool</td>\n","      <td>Normal</td>\n","      <td>Weak</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>D6</td>\n","      <td>Rain</td>\n","      <td>Cool</td>\n","      <td>Normal</td>\n","      <td>Strong</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>D7</td>\n","      <td>Overcast</td>\n","      <td>Cool</td>\n","      <td>Normal</td>\n","      <td>Strong</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>D8</td>\n","      <td>Sunny</td>\n","      <td>Mild</td>\n","      <td>High</td>\n","      <td>Weak</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>D9</td>\n","      <td>Sunny</td>\n","      <td>Cool</td>\n","      <td>Normal</td>\n","      <td>Weak</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>D10</td>\n","      <td>Rain</td>\n","      <td>Mild</td>\n","      <td>Normal</td>\n","      <td>Weak</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>D11</td>\n","      <td>Sunny</td>\n","      <td>Mild</td>\n","      <td>Normal</td>\n","      <td>Strong</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>D12</td>\n","      <td>Overcast</td>\n","      <td>Mild</td>\n","      <td>High</td>\n","      <td>Strong</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>D13</td>\n","      <td>Overcast</td>\n","      <td>Hot</td>\n","      <td>Normal</td>\n","      <td>Weak</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>D14</td>\n","      <td>Rain</td>\n","      <td>Mild</td>\n","      <td>High</td>\n","      <td>Strong</td>\n","      <td>No</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    day   outlook  temp humidity    wind play\n","0    D1     Sunny   Hot     High    Weak   No\n","1    D2     Sunny   Hot     High  Strong   No\n","2    D3  Overcast   Hot     High    Weak  Yes\n","3    D4      Rain  Mild     High    Weak  Yes\n","4    D5      Rain  Cool   Normal    Weak  Yes\n","5    D6      Rain  Cool   Normal  Strong   No\n","6    D7  Overcast  Cool   Normal  Strong  Yes\n","7    D8     Sunny  Mild     High    Weak   No\n","8    D9     Sunny  Cool   Normal    Weak  Yes\n","9   D10      Rain  Mild   Normal    Weak  Yes\n","10  D11     Sunny  Mild   Normal  Strong  Yes\n","11  D12  Overcast  Mild     High  Strong  Yes\n","12  D13  Overcast   Hot   Normal    Weak  Yes\n","13  D14      Rain  Mild     High  Strong   No"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","df_play_tennis = pd.read_csv('../datasets/play_tennis.csv')\n","df_play_tennis"]},{"cell_type":"markdown","metadata":{},"source":["### Priors"]},{"cell_type":"markdown","metadata":{},"source":["First, let us compute the estimates for the prior probabilites.\n","\n","$$\n","\\Pr(\\operatorname{play} = \\text{Yes}) \\approx \\frac{5}{14} \\approx 36\\%\n","$$\n","\n","$$\n","\\Pr(\\operatorname{play} = \\text{No}) \\approx \\frac{9}{14} \\approx 64\\%\n","$$\n","\n","The way to interpret these prior probabilities is the following: if you do not know anything about the weather conditions in a given day, then there is approximately 64% chance that this day is appropriate to play tennis.\n","\n","In general, to compute estimates for the prior probabilities, we use the following expression:\n","\n","$$\n","\\Pr(c_j) \\approx \\frac{q_j}{n}\n","$$\n","\n","In the above expression,\n","\n","- $q_j$ is the number of training examples that belong to class $c_j$;\n","- $n$ is the total number of training examples."]},{"cell_type":"markdown","metadata":{},"source":["### Likelihoods\n","\n","See a nice explanation about conditional probabilities [here](https://setosa.io/conditional/).\n","\n","Recall the estimate $\\Pr(\\operatorname{play} = \\text{No}  \\mid \\operatorname{outlook} = \\text{Sunny}) \\approx 60\\%$. This estimate tells us that, if you are in a sunny day, then the chance is $60$% that this is not a good day to play tennis. Now, compare this value with the estimate for $\\Pr(\\operatorname{play} = \\text{No}) \\approx 36\\%$. We can conclude that, knowing that we are in a sunny day changes our bets that this day is appropriate to play tennis. In other words, it seems to exist a **dependence** between variables `play` and `outlook`. \n","\n","In general, two events $A$ and $B$ are said to be independent if and only if both identities below are true:\n","\n","1. $\\Pr(A \\mid B) = \\Pr(A)$\n","2. $\\Pr(B \\mid A) = \\Pr(B)$\n","\n","We can also easily compute estimates for the conditional probabilities from the data. Somes examples:\n","- $\\Pr(\\operatorname{outlook} = \\text{Sunny}  \\mid \\operatorname{play} = \\text{No}) \\approx \\frac{3}{5}.$\n","- $\\Pr(\\operatorname{outlook} = \\text{Sunny} \\text{ and } \\operatorname{temp} = \\text{Hot} \\mid \\operatorname{play} = \\text{No}) \\approx \\frac{2}{5} = 40\\%$\n","- $\\Pr(\\operatorname{play} = \\text{No}  \\mid \\operatorname{outlook} = \\text{Sunny}) \\approx \\frac{3}{5} = 60\\%$\n","\n","As an exercise, compute estimates for the following conditional probatilities (likelihoods):\n","\n","- $\\Pr(\\text{outlook} = \\text{Sunny} \\mid \\text{play} = \\text{Yes})$\n","- $\\Pr(\\text{outlook} = \\text{Sunny} \\mid \\text{play} = \\text{No})$ \n","\n","- $\\Pr(\\text{temp} = \\text{Hot} \\mid \\text{play} = \\text{Yes})$ \n","- $\\Pr(\\text{temp} = \\text{Hot} \\mid \\text{play} = \\text{No})$ \n","\n","- $\\Pr(\\text{humidity} = \\text{High} \\mid \\text{play} = \\text{Yes})$\n","- $\\Pr(\\text{humidity} = \\text{High} \\mid \\text{play} = \\text{No})$ \n","\n","- $\\Pr(\\text{wind} = \\text{Weak} \\mid \\text{play} = \\text{Yes})$ \n","- $\\Pr(\\text{wind} = \\text{Weak} \\mid \\text{play} = \\text{No})$\n","\n","In general, to compute estimates for the conditional probabilities, we use the following expression:\n","\n","$$\n","\\Pr(x_i \\mid c_j) \\approx \\frac{q_{ij}}{q_j}\n","$$\n","\n","In the above expression,\n","\n","- $q_j$ is the number of training examples that belong to class $c_j$;\n","- $q_{ij}$ is the number of training examples that belong to class $c_j$ and have the provided value for $x_i$ (the i-th attribute)."]},{"cell_type":"markdown","metadata":{},"source":["## Implementation details\n","\n","### Laplace Smoothing\n","\n","So far, we know that the calculation of probability estimates in the NBC method is based on frequency counts over the training dataset $\\mathcal{D}$. For example, to obtain the estimate for the prior probabilities $\\Pr(c_{j})$, it is necessary to determine how often we find examples that belong to class \n","$c_{j}$. \n","\n","However, there is an additional complication in calculating the estimates for the conditional probabilities: zero frequencies cause the estimate of the conditional probability to be zero. To understand this, realize that it suffices for one of the factors in the equation to be zero for the entire product to also be zero.\n","\n","$$\n","\\Pr(x_1, x_2, \\dots x_n \\mid c_j) = \\Pr(x_1 \\mid c_j) \\times \\Pr(x_2 \\mid c_j) \\times \\ldots \\times \\Pr(x_n \\mid c_j)\n","$$\n","\n","In particular, the frequency $\\frac{q_{ij}}{q_{i}}$ is zero when the value used for attribute $x_i$ does not occur in training exmplaes labeled with class $c_j$\n","(because, in this case, $q_{ij} = 0$). To prevent the occurrence of zero frequencies, we must *smooth* the estimates.\n","\n","In general, the procedure of smoothing a probability estimate $e$ means adding a small, positive constant $\\delta$ to it, such that the new estimate is $e+\\delta$. The result is that probability estimates that are zero become greater than zero.\n","\n","One of the techniques used to smooth probability estimates is **Laplace Smoothing**. Remember that the expression to compute the estimates for is the following:\n","$$\n","\t\\Pr(x_{i} \\mid c_{j}) \\approx \\frac{q_{ij}}{q_{j}}\n","$$\n","\n","When applied to the above expression, the Laplace Smoothing technique allows it to be rewritten, as shown below.\n","$$\n","\t\\Pr(x_{i} \\mid c_{j}) \\approx \\frac{q_{ij} + \\lambda}{q_{j} + \\lambda |\\mathcal{C}|}\n","$$\n","\n","- $|\\mathcal{C}|$ denotes the number of different values the target. \n","- Usually $\\lambda$ in the formula is set to 1."]},{"cell_type":"markdown","metadata":{},"source":["Let us present a numerical example of computing the probability estimates using Laplace Smoothing trick. For this, consider the following question: \n","\n","> Is it appropriate or not to play tennis on a sunny, hot, high humidity and light wind day?\n","\n","This question is equivalent to classifying an example $\\mathbf{x}$ corresponding to $[\\operatorname{outlook} = \\text{Sunny}, \\operatorname{temp} = \\text{Hot}, \\operatorname{humidity} = \\text{High}, \\operatorname{wind} = \\text{Weak}]$. To answer this question, we can apply the Naive Bayes classifier to compute $c_{\\text{map}}$. But, before this, we to compute probability estimates for the prior and likelihoods. \n","\n","For the **prior propabilities**, we find that:\n","\n","$$\\Pr(\\operatorname{play} = \\text{Yes}) \\approx \\frac{q_{\\text{Yes}}}{n} = \\frac{9}{14}$$ \n","\n","$$\\Pr(\\operatorname{play} = \\text{No}) \\approx \\frac{q_{\\text{No}}}{n} = \\frac{5}{14}$$\n","\n","Similarly, estimates for **conditional probabilities** $\\Pr(x_i \\mid c_j)$ can be computed. Let us assume we want to compute these probabilities for the value of $\\text{Sunny}$ of the attribute $\\text{outlook}$. If we set the Laplace smoothing parameter to $\\lambda=1$, then:\n","\n","$$\n","\\begin{align*}\n","\\Pr(\\operatorname{outlook} &= \\text{Sunny} \\mid \\operatorname{play} = \\text{Yes}) &\\approx \\frac{q_{ij} + \\lambda}{q_{j} + \\lambda \\mid x_i \\mid} = \\frac{2 + 1}{9 + 1 \\cdot 2} = \\frac{3}{11} \\approx 0.27\\\\\n","\n","\\Pr(\\operatorname{outlook} &= \\text{Sunny} \\mid \\operatorname{play} = \\text{No}) &\\approx \\frac{q_{ij} + \\lambda}{q_{j} + \\lambda \\mid x_i \\mid} = \\frac{3 + 1}{5 + 1 \\cdot 2} = \\frac{4}{7} \\approx 0.57\n","\n","\\end{align*}\n","$$"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0.2727272727272727\n","0.5714285714285714\n"]}],"source":["def laplace_smoothed_probability(q_j, q_i_j, num_values_target, smoothing_parameter=1):\n","  \"\"\"\n","  This function calculates the Laplace-smoothed estimate of a probability.\n","\n","  Args:\n","      q_j (int): The total number of examples associated to target c_j.\n","      q_i_j (int): The total number of examples associated to target c_j, and that have a particular value for predictor x_i.\n","      num_values_target: number of values the target can assume.\n","      smoothing_parameter (int, optional): The smoothing parameter (default 1).\n","\n","  Returns:\n","      float: The Laplace-smoothed probability estimate.\n","  \"\"\"\n","  return (q_i_j + smoothing_parameter) / (q_j + smoothing_parameter * num_values_target)\n","\n","print(laplace_smoothed_probability(9, 2, 2))\n","print(laplace_smoothed_probability(5, 3, 2))"]},{"cell_type":"markdown","metadata":{},"source":["### Log-transformation\n","\n","In addition to Laplace smoothing, another implementation trick is commonly used in Naive Bayes Classifiers (NBC). Consider again the expression used to compute $\\Pr(x_1, x_2, \\dots x_n \\mid y)$. Note that this equation presents a product over the factors $\\Pr(x_{i} \\mid c_{j})$. From a computational point of view, this product represents a complication, because the values $\\Pr(x_{i}|c_{j})$ are typically very close to $0$, which makes their product even closer to zero. Considering that computers have a finite capacity for representing real numbers, this can lead to approximation errors.\n","\n","To circumvent this issue, we leverage a property of the $\\arg\\max$ operator and the logarithmic function $f(x) = \\log(x)$, as described below.\n","\n","First, note that the function $\\log(x)$ is *monotonically increasing*, which means that if  $x_1 \\geq x_2$, then $\\log(x_1) \\geq \\log(x_2)$. Therefore, if we apply the logarithmic function to each element of the list passed as an argument to the argmax operator, the result produced by this operator remains the same. In other words:\n","\n","$$\n","\\underset{c_{j} \\in \\mathcal{C}}{\\operatorname{argmax}} \\left[f(c_j)\\right] = \\underset{c_{j} \\in \\mathcal{C}}{\\operatorname{argmax}} \\left[\\log f(c_j)) \\right]\n","$$\n","\n","It is also worth noting that we can exploit a property of the logarithm function, which states that the logarithm of a product is equal to the sum of the logarithms. As a result, we can transform the product in the original expression for $c_{map}$ into a sum. This is advantageous from a computational standpoint, since sums are less prone to numerical approximation errors than products. This development is presented below.\n","\n","$$\n","\\begin{align*}\n","c_{map} &= \\underset{c_{j} \\in \\mathcal{C}}{\\operatorname{argmax}} \\left[ \\Pr(c_{j}) \\times \\prod_{i}\\Pr(x_i|c_{j})\\right] \\\\ \n","\t\t&= \\underset{c_{j} \\in \\mathcal{C}}{\\operatorname{argmax}} \\log \\left[ \\Pr(c_{j}) \\times \\prod_{i}\\Pr(x_i|c_{j})\\right] \\\\ \n","\t\t&= \\underset{c_{j} \\in \\mathcal{C}}{\\operatorname{argmax}} \\left[ \\log \\Pr(c_{j}) + \\log \\prod_{i}\\Pr(x_i|c_{j})\\right] \\\\ \n","\t\t&= \\underset{c_{j} \\in \\mathcal{C}}{\\operatorname{argmax}} \\left[ \\log \\Pr(c_{j}) + \\sum_{j} \\log \\Pr(x_i|c_{j})\\right] \\\\ \n","\\end{align*}\n","$$\n","\n","Based on the description provided so far, we can conclude that the most probable class for an example $\\mathbf{x}$ can be determined as follows, given the estimates $\\Pr(c_{j})$ e $\\Pr(x_i|c_{j})$:\n","\n","$$\n","\\begin{equation*}\n","\t\tc_{map} = \\underset{c_{j} \\in \\mathcal{C}}{\\operatorname{argmax}} \\left[ \\log \\Pr(c_{i}) + \\sum_{i} \\log \\Pr(x_i|c_{j})\\right]\n","\\end{equation*}\n","$$\n","\n","If we combina both implementation tricks (Laplace Smoothing and Log-Transformation), we get the final expression for $c_{map}$:\n","$$\n","\\begin{equation*}\n","\t\tc_{map} = \\underset{c_{j} \\in \\mathcal{C}}{\\operatorname{argmax}} \\left[\\log \\frac{q_j}{n} + \\sum_{i} \\log \\frac{q_{ij} + \\lambda}{q_{j} + \\lambda |\\mathcal{C}|}\\right]\n","\\end{equation*}\n","$$"]},{"cell_type":"markdown","metadata":{},"source":["## *Recommended* object-oriented design"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["import pandas as pd\n","\n","class NaiveBayesClassifier:\n","  \"\"\"\n","  A class implementing the Naive Bayes Classifier algorithm.\n","  \"\"\"\n","\n","  def __init__(self, smoothing_parameter: int=1):\n","    \"\"\"\n","    Initializes the classifier with a smoothing parameter for Laplace Smoothing.\n","\n","    Args:\n","        smoothing_parameter (int, optional): The smoothing parameter (default 1).\n","    \"\"\"\n","    self.smoothing_parameter = smoothing_parameter\n","    self.classes_ = None\n","    self.feature_counts_ = None  # Dictionary to store feature counts\n","\n","  def fit(self, X: pd.DataFrame, y: pd.Series):\n","    \"\"\"\n","    Fits the classifier to the training data.\n","\n","    Args:\n","        X (pd.DataFrame): The training data features.\n","        y (pd.Series): The training data labels.\n","    \"\"\"\n","    self.classes_ = np.unique(y)\n","    self.feature_counts_ = self._calculate_feature_counts(X, y)\n","\n","\n","  def predict(self, X: pd.DataFrame):\n","    \"\"\"\n","    Predicts the class labels for new data points.\n","\n","    Args:\n","        X (pd.DataFrame): The data points to predict labels for.\n","\n","    Returns:\n","        numpy.ndarray: The predicted class labels for each data point.\n","    \"\"\"\n","    if self.classes_ is None:\n","      raise ValueError(\"Model not fitted yet. Call fit(X, y) first.\")\n","    return np.apply_along_axis(self._predict_proba, axis=1, arr=X.values).argmax(axis=1)\n","\n","  def predict_proba(self, X: pd.DataFrame):\n","    \"\"\"\n","    Predicts the class probabilities for new data points.\n","\n","    Args:\n","        X (pd.DataFrame): The data points to predict class probabilities for.\n","\n","    Returns:\n","        numpy.ndarray: The predicted class probabilities for each data point.\n","    \"\"\"\n","    if self.classes_ is None:\n","      raise ValueError(\"Model not fitted yet. Call fit(X, y) first.\")\n","    return np.apply_along_axis(self._predict_proba, axis=1, arr=X)\n","\n","  def _calculate_feature_counts(self, X, y):\n","    \"\"\"\n","    Calculates the counts of features for each class using Laplace Smoothing.\n","\n","    Args:\n","        X (pd.DataFrame): The training data features.\n","        y (pd.Series): The training data labels.\n","\n","    Returns:\n","        dict: A dictionary containing feature counts for each class.\n","    \"\"\"\n","    ##########################################################################\n","    # YOUR CODE HERE\n","    # ... (Implementation of counting features)\n","    ##########################################################################\n","\n","  def _predict_proba(self, x):\n","    \"\"\"\n","    Calculates the class probabilities for a single data point using Bayes' theorem.\n","\n","    Args:\n","        x (numpy.ndarray): A single data point.\n","\n","    Returns:\n","        numpy.ndarray: The class probabilities for the data point.\n","    \"\"\"\n","    ##########################################################################\n","    # YOUR CODE HERE\n","    # ... (Implementation of Naive Bayes probability calculation with Laplace smoothing)\n","    ##########################################################################\n"]},{"cell_type":"markdown","metadata":{},"source":["#### Additional information on the ``self.feature_counts_`` property:\n","\n","- Inside the ``__init__`` method, we define an empty dictionary {} and assign it to the ``self.feature_counts_`` property.\n","- This dictionary will be used to store the calculated feature counts for each class during the training process (``fit`` method).\n","- The key of the dictionary will be a tuple representing a specific feature and its value (e.g., (\"Outlook\", \"Sunny\")).\n","- The value of the dictionary will be another dictionary that stores counts for each class (e.g., {\"Yes\": 2, \"No\": 3}).\n","- This dictionary allows efficient storage and retrieval of feature counts categorized by class and feature value during prediction.\n","\n","The piece of code below is meant to provived a more concrete understanding of that property. "]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'Yes': 2, 'No': 3}\n","2\n","3\n"]}],"source":["feature_counts_ = {}\n","feature_counts_[(\"outlook\", \"Sunny\")] = {\"Yes\": 2, \"No\": 3}\n","print(feature_counts_[(\"outlook\", \"Sunny\")])\n","print(feature_counts_[(\"outlook\", \"Sunny\")][\"Yes\"])\n","print(feature_counts_[(\"outlook\", \"Sunny\")][\"No\"])"]},{"cell_type":"markdown","metadata":{},"source":["#### Additional information on the function ``numpy.apply_along_axis``.\n","\n","The following example demonstrates how ``numpy.apply_along_axis`` can be used with a pandas DataFrame to perform element-wise operations on each row or column using a custom function."]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["   col1  col2  col3\n","0     1     5   0.1\n","1     2     6   0.2\n","2     3     7   0.3\n","3     4     8   0.4\n","   col1  col2  col3\n","0   1.0  25.0  0.01\n","1   4.0  36.0  0.04\n","2   9.0  49.0  0.09\n","3  16.0  64.0  0.16\n"]}],"source":["import pandas as pd\n","import numpy as np\n","\n","# Sample DataFrame\n","data = {'col1': [1, 2, 3, 4], 'col2': [5, 6, 7, 8], 'col3': [.1, .2, .3, .4]}\n","df = pd.DataFrame(data)\n","\n","# Function to square each element\n","def square(x):\n","  return x * x\n","\n","# Apply square function to each row (axis=0) using numpy.apply_along_axis\n","squared_df = pd.DataFrame(np.apply_along_axis(square, axis=0, arr=df.values))\n","\n","# Set column names for the resulting DataFrame\n","squared_df.columns = df.columns\n","\n","print(df)\n","print(squared_df)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyO0OYOmFCtRxw4kXkW9Yqqn","collapsed_sections":["TLUdfI95-XSR","h4XIbL5FIClk","6AloWTjQjV4E","-TfecgGzmBqu"],"name":"DataMining2020-week09.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":0}

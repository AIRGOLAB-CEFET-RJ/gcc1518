{"cells":[{"cell_type":"markdown","metadata":{"id":"p_Wx4vawBl9n"},"source":["The following image illustrates the effect of applying the function `train_test_split` on the data matrix $X$ and response vector $y$. From these, two data matrices and two response vectors are created. \n","\n","![alt text](https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1543836883/image_6_cfpjpr.png)\n","\n","Please notice that, each time `train_test_split` is called, it randomly selects which examples to put in the training and test datasets."]},{"cell_type":"markdown","metadata":{"id":"GBmQS9s945nl"},"source":["# Naive Bayes Classifier\n","\n","Naive Bayes Classifier is a probabilistic classification model. The model generated by the Naive Bayes algorithm is a set of *conditional probabilities*."]},{"cell_type":"markdown","metadata":{"id":"EZPpo1nB-8-Y"},"source":["## Estimating probabilities from data\n","\n","Let us see an example of how probability estimates can be computed from data provided as a dataset. For this, consider the [Play Tennis dataset](https://www.kaggle.com/fredericobreno/play-tennis), which is another toy dataset with four predictors (`outlook`, `temp`, `humidity`, and `wind`) and fourteen examples. The target (`play`) is binary. Each example provides data about the weather condition in a particular day. Therefore, the classification task is to predict whether a given day is appropriate to play tennis or not."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":468},"id":"3I5IkDcb68a6","outputId":"032ded88-d686-48cb-dac8-5da96297a871"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>day</th>\n","      <th>outlook</th>\n","      <th>temp</th>\n","      <th>humidity</th>\n","      <th>wind</th>\n","      <th>play</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>D1</td>\n","      <td>Sunny</td>\n","      <td>Hot</td>\n","      <td>High</td>\n","      <td>Weak</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>D2</td>\n","      <td>Sunny</td>\n","      <td>Hot</td>\n","      <td>High</td>\n","      <td>Strong</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>D3</td>\n","      <td>Overcast</td>\n","      <td>Hot</td>\n","      <td>High</td>\n","      <td>Weak</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>D4</td>\n","      <td>Rain</td>\n","      <td>Mild</td>\n","      <td>High</td>\n","      <td>Weak</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>D5</td>\n","      <td>Rain</td>\n","      <td>Cool</td>\n","      <td>Normal</td>\n","      <td>Weak</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>D6</td>\n","      <td>Rain</td>\n","      <td>Cool</td>\n","      <td>Normal</td>\n","      <td>Strong</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>D7</td>\n","      <td>Overcast</td>\n","      <td>Cool</td>\n","      <td>Normal</td>\n","      <td>Strong</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>D8</td>\n","      <td>Sunny</td>\n","      <td>Mild</td>\n","      <td>High</td>\n","      <td>Weak</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>D9</td>\n","      <td>Sunny</td>\n","      <td>Cool</td>\n","      <td>Normal</td>\n","      <td>Weak</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>D10</td>\n","      <td>Rain</td>\n","      <td>Mild</td>\n","      <td>Normal</td>\n","      <td>Weak</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>D11</td>\n","      <td>Sunny</td>\n","      <td>Mild</td>\n","      <td>Normal</td>\n","      <td>Strong</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>D12</td>\n","      <td>Overcast</td>\n","      <td>Mild</td>\n","      <td>High</td>\n","      <td>Strong</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>D13</td>\n","      <td>Overcast</td>\n","      <td>Hot</td>\n","      <td>Normal</td>\n","      <td>Weak</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>D14</td>\n","      <td>Rain</td>\n","      <td>Mild</td>\n","      <td>High</td>\n","      <td>Strong</td>\n","      <td>No</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    day   outlook  temp humidity    wind play\n","0    D1     Sunny   Hot     High    Weak   No\n","1    D2     Sunny   Hot     High  Strong   No\n","2    D3  Overcast   Hot     High    Weak  Yes\n","3    D4      Rain  Mild     High    Weak  Yes\n","4    D5      Rain  Cool   Normal    Weak  Yes\n","5    D6      Rain  Cool   Normal  Strong   No\n","6    D7  Overcast  Cool   Normal  Strong  Yes\n","7    D8     Sunny  Mild     High    Weak   No\n","8    D9     Sunny  Cool   Normal    Weak  Yes\n","9   D10      Rain  Mild   Normal    Weak  Yes\n","10  D11     Sunny  Mild   Normal  Strong  Yes\n","11  D12  Overcast  Mild     High  Strong  Yes\n","12  D13  Overcast   Hot   Normal    Weak  Yes\n","13  D14      Rain  Mild     High  Strong   No"]},"execution_count":22,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["import pandas as pd\n","df_play_tennis = pd.read_csv('play_tennis.csv')\n","df_play_tennis"]},{"cell_type":"markdown","metadata":{"id":"UU0R25fD2rFT"},"source":["# Bayes' theorem\n","\n","The term \"bayesian\" comes from [Thomas Bayes](https://en.wikipedia.org/wiki/Thomas_Bayes), a British Presbyterian minister who lived in the 18th century and who formulated the famous [Bayes' theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem).\n","\n","$$\n","\\Pr(y \\mid x_1, x_2, \\dots, x_n) = \\frac{\\Pr (y) \\Pr (x_1, x_2, \\dots x_n \\mid y)} {\\Pr(x_1, x_2, \\dots, x_n)}\n","$$\n","\n","The following is a description of each term in the above expression, in the context of a [statistical classification](https://en.wikipedia.org/wiki/Statistical_classification) task:\n","\n","* $\\Pr(y \\mid x_1, \\dots, x_n)$ represents the probability of the class $y$, given the values ​​of the attributes of the example $\\mathbf{x}$. This term, called **posterior probability**, is what must be determined (learned) by the algorithm.\n","\n","* $\\Pr(x_1, \\dots x_n \\mid y)$ represents the probability that a specific combination of values ​​$x_1, \\dots, x_n$ will occur in examples associated with a specific value of the target attribute $y$. This term is called **likelihood**.\n","\n","* $\\Pr(y)$ represents the probability that an example selected at random belongs to a given class (i.e., belongs to a given value of the target attribute $y$). This term is called **prior probability**\n","\n","* $\\Pr (x_1, x_2, \\dots, x_n) $ represents the probability that a given combination of values ​​$ x_1, x_2, \\dots, x_n$ will occur in an example selected at random.\n","\n","These probabilities are actually estimated from the training dataset by the Naive Bayes algorithm. These estimates are computed by counting the occurrences of values in a given feature, either separately  or in conjunction with values of other features."]},{"cell_type":"markdown","metadata":{"id":"I-Au2EO4AD7q"},"source":["# Prior probabilities"]},{"cell_type":"markdown","metadata":{"id":"ckwkHH0fAtJy"},"source":["Let us see some examples of probability estimates that can be computed from the above dataset. First, let us compute the estimates for the prior probabilites.\n","\n","$$\n","\\Pr(\\operatorname{play} = \\text{Yes}) \\approx \\frac{5}{14} \\approx 36\\%\n","$$\n","\n","$$\n","\\Pr(\\operatorname{play} = \\text{No}) \\approx \\frac{9}{14} \\approx 64\\%\n","$$\n","\n","The way to interpret these prior probabilities is the following: if you do not know anything about the weather conditions in a given day, then there is approximately 64% chance that this day is appropriate to play tennis."]},{"cell_type":"markdown","metadata":{"id":"ZZsnGcSjEy75"},"source":["# Conditional probabilities\n","\n","See a nice explanation about conditional probabilities [here](https://setosa.io/conditional/).\n","\n","Recall the estimate $\\Pr(\\operatorname{play} = \\text{No}  \\mid \\operatorname{outlook} = \\text{Sunny}) \\approx 60\\%$. This estimate tells us that, if you are in a sunny day, then the chance is $60$% that this is not a good day to play tennis. Now, compare this value with the estimate for $\\Pr(\\operatorname{play} = \\text{'No'}) \\approx 36\\%$. We can conclude that, knowing that we are in a sunny day changes our bets that this day is appropriate to play tennis. In other words, it seems to exist a **dependence** between variables `play` and `outlook`. \n","\n","In general, two events $A$ and $B$ are said to be independent if and only if both identities below are true:\n","\n","1. $\\Pr(A \\mid B) = \\Pr(A)$\n","2. $\\Pr(B \\mid A) = \\Pr(B)$\n","\n","We can also easily compute estimates for the conditional probabilities from the data. Somes examples:\n","- $\\Pr(\\operatorname{outlook} = \\text{Sunny}  \\mid \\operatorname{play} = \\text{No}) \\approx \\frac{3}{5}.$\n","- $\\Pr(\\operatorname{outlook} = \\text{Sunny} \\text{ and } \\operatorname{temp} = \\text{Hot} \\mid \\operatorname{play} = \\text{No}) \\approx \\frac{2}{5} = 40\\%$\n","- $\\Pr(\\operatorname{play} = \\text{No}  \\mid \\operatorname{outlook} = \\text{Sunny}) \\approx \\frac{3}{5} = 60\\%$\n","\n","As an exercise, compute estimates for the following conditional probatilities (likelihoods):\n","\n","- $\\Pr(\\text{outlook} = \\text{Sunny} \\mid \\text{play} = \\text{Yes})$\n","- $\\Pr(\\text{outlook} = \\text{Sunny} \\mid \\text{play} = \\text{No})$ \n","\n","- $\\Pr(\\text{temp} = \\text{Hot} \\mid \\text{play} = \\text{Yes})$ \n","- $\\Pr(\\text{temp} = \\text{Hot} \\mid \\text{play} = \\text{No})$ \n","\n","- $\\Pr(\\text{humidity} = \\text{High} \\mid \\text{play} = \\text{Yes})$\n","- $\\Pr(\\text{humidity} = \\text{High} \\mid \\text{play} = \\text{No})$ \n","\n","- $\\Pr(\\text{wind} = \\text{Weak} \\mid \\text{play} = \\text{Yes})$ \n","- $\\Pr(\\text{wind} = \\text{Weak} \\mid \\text{play} = \\text{No})$"]},{"cell_type":"markdown","metadata":{"id":"ssZtdQAdM119"},"source":["## Naive Bayes Classifier - derivation of the algorithm\n","\n","Naive Bayes Classifier is an algorithm consisting of two steps, which are described below. Formally, let $X$ be a dataset. Also consider that $c_1, c_2, \\ldots, c_k$ are the classes of the problem (i.e., the possible values ​​of the target) and that $\\mathbf{x} = [x_1, x_2, ..., x_n]$ is a new example that should be classified. Let $a_1, a_2, ..., a_n$ be the values for the predictive features $x_1, x_2, ..., x_n$, respectively. \n","\n","NBC allows us to compute the probability that a new example $\\mathbf{x}$ belongs to each of the classes (i.e. each of the possible values for the target). More concretely, consider that $\\mathbf{x}$ is the following:\n","\n","$$\n","\\mathbf{x} = [\\operatorname{outlook} = \\text{Sunny}, \\operatorname{temp} = \\text{Hot}, \\operatorname{humidity} = \\text{High}, \\operatorname{wind} = \\text{Weak}]\n","$$\n","\n","Hence, we are provided information about the weather conditions of a given day (represented by $\\mathbf{x}$), and we want to answer (*predict*) whether or not this is a good day to play tennis. \n","\n","There are only two possibilities, Yes (play=Yes) or No (play=No). Therefore, let us define two probability values:\n","\n","$$\n","\\begin{align*}\n","\\Pr(\\text{play = Yes} \\mid \\mathbf{x}) & \\text{: probability that } \\mathbf{x} \\text{ is a good day to play.}\\\\\n","\\Pr(\\text{play = No} \\mid \\mathbf{x}) & \\text{: probability that } \\mathbf{x} \\text{ is NOT a good day to play.}\n","\\end{align*}\n","$$\n","\n","Notice that, if we know the probabilites values above, the problem is solved. That is because we can use these values to make our decision: if $\\Pr(\\text{play = Yes} > \\Pr(\\text{play = No})$, then we predict that $\\mathbf{x}$ is a good day to play tennis, that is, we predict play = Yes. Otherwise, we predict play = No.\n","\n","But, how can we compute those probability values? The answer lies in the Bayes Rule. To compute $\\Pr(\\text{play = Yes} \\mid \\mathbf{x})$, we use Bayes Rules and write:\n","\n","$$\n","\\begin{align*}\n","\\Pr(\\text{play = Yes} \\mid \\mathbf{x}) & = \\frac{\\Pr(\\mathbf{x} \\mid \\text{play = Yes}) \\times \\Pr(\\text{play} = \\text{Yes})}{\\Pr(\\mathbf{x})} = \\\\\n","&=  \\frac{\\Pr(\\text{outlook} = \\text{Sunny}, \\operatorname{temp} = \\text{Hot}, \\operatorname{humidity} = \\text{High}, \\operatorname{wind} = \\text{Weak} \\mid \\text{play = Yes}) \\times \\Pr(\\text{play = Yes})}{\\Pr(\\text{outlook} = \\text{Sunny}, \\operatorname{temp} = \\text{Hot}, \\operatorname{humidity} = \\text{High}, \\operatorname{wind} = \\text{Weak})}\n","\\end{align*}\n","$$\n","\n","To compute $\\Pr(\\text{play = No} \\mid \\mathbf{x})$, we write a similar expression:\n","\n","$$\n","\\begin{align*}\n","\\Pr(\\text{play = No} \\mid \\mathbf{x}) & = \\frac{\\Pr(\\mathbf{x} \\mid \\text{play = No}) \\times \\Pr(\\text{play} = \\text{No})}{\\Pr(\\mathbf{x})} = \\\\\n","&=  \\frac{\\Pr(\\text{outlook} = \\text{Sunny}, \\operatorname{temp} = \\text{Hot}, \\operatorname{humidity} = \\text{High}, \\operatorname{wind} = \\text{Weak} \\mid \\text{play = No}) \\times \\Pr(\\text{play = No})}{\\Pr(\\text{outlook} = \\text{Sunny}, \\operatorname{temp} = \\text{Hot}, \\operatorname{humidity} = \\text{High}, \\operatorname{wind} = \\text{Weak})}\n","\\end{align*}\n","$$\n","\n","By looking at the two expressions above, it seems there are several probability values we need to compute using the provided dataset. Let us list each one of them:\n","\n","1. $\\Pr(\\text{play = Yes})$\n","2. $\\Pr(\\text{play = No})$\n","3. $\\Pr(\\text{outlook} = \\text{Sunny}, \\operatorname{temp} = \\text{Hot}, \\operatorname{humidity} = \\text{High}, \\operatorname{wind} = \\text{Weak} \\mid \\text{play = No})$\n","4. $\\Pr(\\text{outlook} = \\text{Sunny}, \\operatorname{temp} = \\text{Hot}, \\operatorname{humidity} = \\text{High}, \\operatorname{wind} = \\text{Weak} \\mid \\text{play = Yes})$\n","5. $\\Pr(\\text{outlook} = \\text{Sunny}, \\operatorname{temp} = \\text{Hot}, \\operatorname{humidity} = \\text{High}, \\operatorname{wind} = \\text{Weak})$\n","\n","Items 1 and 2 are easy to compute, and we arlready know how to estimate them. Another good news is that we don't actually need to compute item 5, since this expression appears as denominator of both $\\Pr(\\text{play = yes} \\mid \\mathbf{x})$ and $\\Pr(\\text{play = No} \\mid \\mathbf{x})$. We are left with items 3 and 4. Lets us apply the definition of [conditional probability](https://en.wikipedia.org/wiki/Conditional_probability) to one of these expressions (item 4):\n","\n","$$\n","\\begin{align*}\n","\\Pr(\\text{outlook} = \\text{Sunny}, \\operatorname{temp} = \\text{Hot}, \\operatorname{humidity} = \\text{High}, \\operatorname{wind} = \\text{Weak} \\mid \\text{play = Yes}) = \\\\\n","\\Pr(\\text{outlook = Sunny} \\mid \\text{play = Yes}) \\times \\\\ \n","\\times\\Pr(\\operatorname{temp} = \\text{Hot} \\mid \\text{outlook} = \\text{Sunny}, \\text{play = Yes}) \\times \\\\\n","\\times\\Pr(\\operatorname{humidity} = \\text{High} \\mid \\text{outlook} = \\text{Sunny}, \\operatorname{temp} = \\text{Hot}, \\text{play = Yes}) \\times\\\\\n","\\times\\Pr(\\operatorname{wind} = \\text{Weak} \\mid \\text{outlook} = \\text{Sunny}, \\operatorname{temp} = \\text{Hot}, \\operatorname{humidity} = \\text{High}, \\text{play = Yes})\n","\\end{align*}\n","$$\n","\n","By looking at the above expression, it seems that we have to compute a lot of probability estimates from the available data. That is when the naive assumption of Naive Bayes Classifier comes in handy. This algorithm assumes that one attribute is conditionally independent from each other, once we know the value of the class. \n","\n","> *Conditional independence*. The term [conditional independence](https://www.probabilitycourse.com/chapter1/1_4_4_conditional_independence.php) corresponds to a somewhat advanced concept in Probability Theory. Given three variables A, B, and C. We say that variables A and B are conditionally independet given the variable C if and only if knowing the value of C makes A and B independent of each other.\n","$$\n","\\Pr(A \\mid B, C) = \\Pr(A \\mid C)\n","$$\n","\n","The term *naive* stems from the fact that Naive Bayes considers that the attributes are conditionally independent given the class. When considering this hypothesis, the computation of the conditional probabilities can be simplified. Mathematically, we have:\n","$$\n","\\Pr(x_1, x_2, \\dots x_n \\mid y) = \\Pr(x_1 \\mid y) \\times \\Pr(x_2 \\mid y) \\times \\ldots \\times \\Pr(x_n \\mid y)\n","$$\n","\n","In many practical cases, this statistical independence between predictors does not exist. For example, consider a dataset with information about customers of a company. Also consider that each customer is represented by the following features: *weight*, *education*, *salary*, *age*, etc. In this dataset, the values ​​of the first three feature are correlated with values ​​of the age. In this case, at least in theory, the use of Naive Bayes would overestimate the effect of the age feature. However, practice shows that Naive Bayes is quite effective even in cases where the predictive features are not statistically independent.\n","\n","Anyway, assuming the naive hypothesis is true, we can simplify the Bayes formula:\n","\n","$$\n","\\Pr(y \\mid x_1, x_2, \\dots, x_n) \\propto \\Pr(y) \\times \\Pr(x_1 \\mid y) \\times \\Pr(x_2 \\mid y) \\times \\ldots \\Pr(x_n \\mid y)\n","$$\n","\n","\n","Naive Bayes Classifier uses this assumption of conditional independence to simplify the computation of the probability estimates tha should be produced. By applying this assumption to the estimates above, we end up with the following:\n","\n","$$\n","\\begin{align*}\n","\\Pr(\\text{outlook} = \\text{Sunny}, \\operatorname{temp} = \\text{Hot}, \\operatorname{humidity} = \\text{High}, \\operatorname{wind} = \\text{Weak} \\mid \\text{play = Yes}) = \\\\\n","\\Pr(\\text{outlook = Sunny} \\mid \\text{play = Yes}) \\times \\\\ \n","\\times\\Pr(\\operatorname{temp} = \\text{Hot} \\mid \\text{play = Yes}) \\times \\\\\n","\\times\\Pr(\\operatorname{humidity} = \\text{High} \\mid \\text{play = Yes}) \\times\\\\\n","\\times\\Pr(\\operatorname{wind} = \\text{Weak} \\mid \\text{play = Yes})\n","\\end{align*}\n","$$\n","\n","We can write a similar expression for $\\text{play = No}$:\n","\n","$$\n","\\begin{align*}\n","\\Pr(\\text{outlook} = \\text{Sunny}, \\operatorname{temp} = \\text{Hot}, \\operatorname{humidity} = \\text{High}, \\operatorname{wind} = \\text{Weak} \\mid \\text{play = No}) = \\\\\n","\\Pr(\\text{outlook = Sunny} \\mid \\text{play = No}) \\times \\\\ \n","\\times\\Pr(\\operatorname{temp} = \\text{Hot} \\mid \\text{play = No}) \\times \\\\\n","\\times\\Pr(\\operatorname{humidity} = \\text{High} \\mid \\text{play = No}) \\times\\\\\n","\\times\\Pr(\\operatorname{wind} = \\text{Weak} \\mid \\text{play = No})\n","\\end{align*}\n","$$\n","\n","$$\n","\\Pr(c_i \\mid \\mathbf{x}), \\, 1 \\leq i \\leq n\n","$$"]},{"cell_type":"markdown","metadata":{},"source":["## Inference (prediction)\n","\n","Steps:\n","\n","1. Calculate the posterior probabilities $\\Pr(c_j \\mid \\mathbf{x})$, $j = 1,2, \\ldots, k $\n","2. Classify $\\mathbf{x}$ as being of class $c$ such that $\\Pr(c \\mid \\mathbf{x})$ is maximum.\n","\n","\n","Therefore, to compute the probability that an example $\\mathbf{x}$ belongs to a given class, we just need the estimates for $\\Pr(y)$ and for $\\Pr(x_i \\mid y)$, which you already know how to compute (see the above examples for the Play Tennis dataset). Together, these probability values represent the model generated by the Naive Bayes algorithm. \n"]},{"cell_type":"markdown","metadata":{"id":"DT__Dg-YTZ_L"},"source":["## Numerical example\n","\n","Let us present a numerical example of applying Naive Bayes classifier to the PlayTennis dataset. For this, consider the following question: \n","\n","> Is it appropriate or not to play tennis on a sunny, hot, high humidity and light wind day?\n","\n","This question is equivalent to classifying an example $\\mathbf{x}$ corresponding to $[\\operatorname{outlook} = \\text{Sunny}, \\operatorname{temp} = \\text{Hot}, \\operatorname{humidity} = \\text{High}, \\operatorname{wind} = \\text{Weak}]$. To answer this question, we can apply the Naive Bayes classifier. \n","\n","For the **prior propabilities**, we find that:\n","\n","- $\\Pr(\\operatorname{play} = \\text{Yes}) \\approx 9/14$ \n","- $\\Pr(\\operatorname{play} = \\text{No}) \\approx 5/14$\n","\n","Similarly, estimates for **conditional probabilities** $\\Pr(x_i \\mid c_j)$ are calculated:\n","\n","- $\\Pr(\\operatorname{outlook} = \\text{Sunny} \\mid \\operatorname{play} = \\text{Yes}) \\approx 5/9$\n","- $\\Pr(\\operatorname{outlook} = \\text{Sunny} \\mid \\operatorname{play} = \\text{No}) \\approx 2/5$\n","\n","- $\\Pr(\\operatorname{temp} = \\text{Hot} \\mid \\operatorname{play} = \\text{Yes}) \\approx 2/9$\n","- $\\Pr(\\operatorname{temp} = \\text{Hot} \\mid \\operatorname{play} = \\text{No}) \\approx 2/5$\n","\n","- $\\Pr(\\operatorname{humidity} = \\text{High} \\mid \\operatorname{play} = \\text{Yes}) \\approx 3/9$\n","- $\\Pr(\\operatorname{humidity} = \\text{High} \\mid \\operatorname{play} = \\text{No}) \\approx 4/5$\n","\n","- $\\Pr(\\operatorname{wind} = \\text{Weak} \\mid \\operatorname{play} = \\text{Yes}) \\approx 6/9$\n","- $\\Pr(\\operatorname{wind} = \\text{Weak} \\mid \\operatorname{play} = \\text{No}) \\approx 2/5$\n","\n","The posterior probabilities $\\Pr(\\operatorname{play} = \\text{Yes} \\mid \\mathbf{x})$ and $\\Pr(\\operatorname{play} = \\text{No} \\mid \\mathbf{x})$ can now be (proportionally)  computed. In the following, we ommit the feature names, for simplicity's sake.\n","\n","For $\\Pr(\\operatorname{play} = \\text{Yes} \\mid \\mathbf{x})$:\n","\n","\\begin{align*}\n","\\Pr(\\operatorname{play} = \\text{Yes} \\mid \\mathbf{x}) & \\propto \\Pr(\\text{Sunny} \\mid \\text{Yes}) \\times \\Pr(\\text{Warm} \\mid \\text{Yes}) \\times \\Pr(\\text{High} \\mid \\text{Yes}) \\times \\Pr(\\text{Wind} \\mid \\text{Yes}) \\times \\Pr(\\text{Yes}) = \\\\\n","&= 0.0071 \\times 9/14 = \\\\\n","&= 0.004564286.\n","\\end{align*}\n","\n","For $\\Pr(\\operatorname{play} = \\text{No} \\mid \\mathbf{x})$:\n","\n","\\begin{align*}\n","\\Pr(\\operatorname{play} = \\text{No} \\mid \\mathbf{x}) & \\propto \\Pr(\\text{Sunny} \\mid \\text{No}) \\times \\Pr(\\text{Warm} \\mid \\text{No}) \\times \\Pr(\\text{High} \\mid \\text{No}) \\times \\Pr(\\text{Wind} \\mid \\text{No}) \\times \\Pr(\\text{No}) = \\\\\n","&= 0.0274 \\times 5/14 = \\\\\n","&= 0.009785714.\n","\\end{align*}\n","\n","Now, since $\\Pr(\\operatorname{play} = \\text{Yes} \\mid \\mathbf{x}) + \\Pr(\\operatorname{play} = \\text{No} \\mid \\mathbf{x})$ = 1$, the numbers aborve can be mapped back to probabilities:\n","\n","\\begin{align*}\n","\\Pr(\\operatorname{play} &= \\text{Yes} \\mid \\mathbf{x}) = \\frac{0.004564286}{0.004564286+0.009785714} \\approx 32\\%\\\\\n","\\\\\n","\\Pr(\\operatorname{play} &= \\text{No} \\mid \\mathbf{x}) = \\frac{0.009785714}{0.004564286+0.009785714} \\approx 68\\%\\end{align*}\n","\n","\n","Since the highest value corresponds to $\\operatorname{play} = \\text{No}$, then the class Naive Bayes predicts for $\\mathbf{x}$ is $\\text{No}$."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"elapsed":687,"status":"ok","timestamp":1594925886453,"user":{"displayName":"Eduardo Bezerra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtLlrVPd1NAOukzvdhFbWNfJNyiPGoaltpqHbN_A=s64","userId":"15338717559428153474"},"user_tz":180},"id":"iQePXIuZ_VJD","outputId":"768620ea-e369-41eb-9c7a-8d5a61eb4495"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.31806871080139376\n","0.6819312891986063\n"]}],"source":["print(0.004564286/(0.004564286+0.009785714))\n","print(0.009785714/(0.004564286+0.009785714))"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyO0OYOmFCtRxw4kXkW9Yqqn","collapsed_sections":["TLUdfI95-XSR","h4XIbL5FIClk","6AloWTjQjV4E","-TfecgGzmBqu"],"name":"DataMining2020-week09.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}
